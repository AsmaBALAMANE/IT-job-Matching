{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModesTrainingl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGRJAbNpYL5a",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://github.com/AsmaBALAMANE/job-profile-matcher\">github repository </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex1TWNicyXrn",
        "colab_type": "text"
      },
      "source": [
        "#IT job-candidate matching\n",
        "> *Data-driven solution for job offers and IT professionals automatic matching*\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZW3YrezfBHP",
        "colab_type": "text"
      },
      "source": [
        "<h1>Project scope</h1>\n",
        "\n",
        "Most job-seekers are using Internet platforms to search for new opportunities and advance their careers. During the early stages of recruitment (pre-interview), reviewing many candidate qualifications, and comparing the required skills may slow the process. It is far from easy to find candidates and match them with the best possible jobs. It takes hours of search to get good matches.\n",
        "\n",
        "\n",
        "The automatic matching system we offer simulates the human resources process to match candidates and job offers.  Usually, when searching for suitable candidates and matching candidates to job offers, human recruiters use their understanding of the meaning of words. Then, depending on their semantic interpretation, they find the perfect match.  \n",
        "<h1>System design</h1> \n",
        "\n",
        "The system takes as input a job offer ‘j’ and a candidate's profile ‘p’. It gives as output a matching rate ‘r’ (0 < r < 1 /  with 0 for no matching and 1 for perfectly matched).\n",
        "\n",
        "\n",
        "A Job offer is divided into multiple sections, the meaningful information could be extracted from the job title, description, and required skills sections;  the same applies to the candidate's profile:  professional title, the candidate experiences, and skills, respectively. \n",
        "<h1>System process</h1> \n",
        "\n",
        "At the first stage, each pair of input data i.e. ( job title, professional candidate title), ( job description, candidate experiences), and (job required skills, candidate skills) is processed by the corresponding model.\n",
        "\n",
        "\n",
        "Models return as output word embeddings. Then, the SIF technique is applied to generate sentence embedding vectors. After that, Vectors Cosine similarity is calculated to define the matching rate for each input pair.  These partial matching rates are weighted by factors (see default values in  Table3 ). The final matching rate 'r' of the job offer and the candidate profile is the weighted average of the partial matching rates  ( m1, m2, and m3).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1LJ-oelzZ39N4jv35R4ELju8hX9g5FyYO)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbQZpSItPahm",
        "colab_type": "text"
      },
      "source": [
        "Requirements and packages Instalation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmF3dnHEllKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  \n",
        "import numpy as np\n",
        "from time import time  \n",
        "import logging  \n",
        "import multiprocessing\n",
        "import collections\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import gensim\n",
        "from gensim.models import Word2Vec, KeyedVectors   \n",
        "# For preprocessing\n",
        "import re\n",
        "from gensim.models.phrases import Phrases, Phraser \n",
        "import spacy  \n",
        "import string\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "# For word frequency \n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity  \n",
        "# for models frozing\n",
        "import pickle\n",
        "# Setting up the loggings to monitor gensim\n",
        "from gensim.test.utils import datapath\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuzQJ0rLO0wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR0pEizPXqSW",
        "colab_type": "text"
      },
      "source": [
        "# Model creation and data processing Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi28BBpg8M4P",
        "colab_type": "text"
      },
      "source": [
        "<h1>Data Processing</h1>\n",
        "\n",
        "Data needs to be cleaned in order to perform further tasks of NLP. This preprocessing phase includes:\n",
        "\n",
        "1.   removing incomplete data rows \n",
        "2.   using regular expressions to remove non-ASCII values, special characters\n",
        "3.   Tokenization i.e. segmentation of the input text into tokens (words)  which can be used for further processing\n",
        "4.   removing stop words\n",
        "5.   named entity recognition (NER) to extract entities from text streams\n",
        "6.   Lemmatization i.e. reducing word variations to simpler forms ( the root word is called lemma)\n",
        "7.   detecting common phrases (bigrams) to extract sequences of tokens (phrases) that have a strong independent meaning of the words when treated separately ex.“Big Data\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1CDdOKglbUBWjoD3lFKEXXFxqKjbHMnOe)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2UAm4eJJMYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning(doc):\n",
        "    # doc is a spacy Doc object\n",
        "    # Lemmatization and  stopwords removing (using the SpaCy stopWords list)\n",
        "    text = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    txt = ' '.join(text)\n",
        "    #remove stopword using gensim stopword list\n",
        "    txt = remove_stopwords(txt)\n",
        "    #correct some indetected words by the NER model \n",
        "    txt= txt.replace(\"c #\", \"c#\").replace(\"phantom js\", \"phantomjs\")\n",
        "    return txt\n",
        "def data_processing(column):\n",
        "  # delete null rows\n",
        "  column = column.dropna().reset_index(drop=True)\n",
        "  # Loading the en_core_web_sm SpaCy model \n",
        "  nlp = spacy.load('en_core_web_sm') \n",
        "   # keeping only words \n",
        "  brief_cleaning = (re.sub(\"[^A-Za-z#++']+\", ' ', str(row)).lower() for row in column)\n",
        "  t = time()\n",
        "  txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
        "  print('Cleaning time: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "  df_clean = pd.DataFrame({'cleaned': txt})\n",
        "  df_clean = df_clean.dropna().drop_duplicates()\n",
        "  sent = [row.split() for row in df_clean['cleaned']]\n",
        "  # bigram creation\n",
        "  phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
        "  bigram = Phraser(phrases)\n",
        "  sentences = bigram[sent]\n",
        "  return sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apQr2b3xvn6j",
        "colab_type": "text"
      },
      "source": [
        "<h1>Model building and training</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbaTpwZWVneq",
        "colab_type": "text"
      },
      "source": [
        "Features Extraction\n",
        "\n",
        "\n",
        "In order to represent sentences as embedding vectors, the system uses Smooth Inverse Frequency (SIF) technique, It takes a weighted average of word embeddings. Every word embedding is weighted by a / (a + p(w)) , where a is a parameter that is typically set to 0.001, and p(w)is the estimated frequency of the word in a corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoiP7DSFCI7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Features Extraction \n",
        "def map_word_frequency(document):\n",
        "    return Counter(itertools.chain(*document))\n",
        "    \n",
        "def get_sif_feature_vectors(sentence1, sentence2, word_emb_model):\n",
        "    sentence1 = [token for token in sentence1 if token in word_emb_model.wv.vocab]\n",
        "    sentence2 = [token for token in sentence2 if token in word_emb_model.wv.vocab]\n",
        "    word_counts = map_word_frequency((sentence1 + sentence2))\n",
        "    embedding_size = 300 # size of vectore in word embeddings\n",
        "    a = 0.001\n",
        "    sentence_set=[]\n",
        "    for sentence in [sentence1, sentence2]:\n",
        "        vs = np.zeros(embedding_size)\n",
        "        sentence_length = len(sentence)\n",
        "        for word in sentence:\n",
        "            a_value = a / (a + word_counts[word]) # smooth inverse frequency, SIF\n",
        "            vs = np.add(vs, np.multiply(a_value, word_emb_model.wv[word])) # vs += sif * word_vector\n",
        "        vs = np.divide(vs, sentence_length) # weighted average\n",
        "        sentence_set.append(vs)\n",
        "    return sentence_set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDsxG_2uoVgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input preparation \n",
        "def input_preparation(sentence):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc1=nlp(sentence)\n",
        "    txt= [token.lower() for token in cleaning(doc1).split()]\n",
        "    txt = list(set(txt)) \n",
        "    r = re.compile(\"[A-Za-z#++']+\")\n",
        "    txt = list(filter(r.match, txt))\n",
        "    phrases = Phrases(txt, min_count=10, progress_per=10000)\n",
        "    bigram = Phraser(phrases)\n",
        "    sentences = bigram[txt]\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSnkxeH3WhbX",
        "colab_type": "text"
      },
      "source": [
        "Word2Vec training\n",
        "\n",
        "Word2vec uses a single hidden layer, fully connected neural network. The neurons in the hidden layer are all linear neurons. The input layer is set to have as many neurons as there are words in the vocabulary for training (V). The hidden layer size is set to the dimensionality (N) of the resulting word vectors. The size of the output layer is the same as the input layer.\n",
        "The input to hidden layer connections can be represented by a matrix (Win) of size (VxN) with each row representing a vocabulary word. In the same way, the connections from the hidden layer to the output layer can be described by a matrix (Wout)of size (NxV). In this case, each column of (Wout) matrix represents a word from the given vocabulary\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1VKPYt2tl2or2lJBsZ4s1Vkng3CS-VBAE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Kea0Bvc_N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#min_count=5 or 20 for description , window=2,size=300,sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20,workers=cores-1\n",
        "def word2vec_creation(data,min_count,window,size,sample,alpha,min_alpha,negative):\n",
        "  # Count the number of cores in a computer\n",
        "  cores = multiprocessing.cpu_count() \n",
        "  w2v_model = Word2Vec(min_count=min_count,\n",
        "                     window=window,\n",
        "                     size=size,\n",
        "                     sample=sample, \n",
        "                     alpha=alpha, \n",
        "                     min_alpha=min_alpha, \n",
        "                     negative=negative,\n",
        "                     workers=cores-1)\n",
        "  t = time()\n",
        "  #build the word2Vec vocabulary\n",
        "  w2v_model.build_vocab(data, progress_per=10000)\n",
        "  print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "  return w2v_model\n",
        "  \n",
        "def word2vec_training(data,w2v_model):\n",
        "  t = time()\n",
        "  w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "  print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "  return w2v_model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAy76Tp2Ws-8",
        "colab_type": "text"
      },
      "source": [
        "Docs Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrRIDIBjGz_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate similarity \n",
        "def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
        "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1meYsQgqUrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def job_profile_matching(skills_rate, title_rate, description_rate, job, profile,skills_model, titles_model, desciptions_model):\n",
        "  \n",
        "  #Skills similarity \n",
        "    job_skills= input_preparation(job[0])\n",
        "    profile_skills= input_preparation(profile[0])\n",
        "    result_skills= get_sif_feature_vectors(job_skills,profile_skills,skills_model)\n",
        "    #if any word from job input or profile input could be detected in the vocabulary\n",
        "    if(result_skills==0):\n",
        "       matching_skills=0\n",
        "    else:\n",
        "       matching_skills= get_cosine_similarity(result_skills[0],result_skills[1])  \n",
        "    print('matching_skills:', matching_skills)\n",
        "  #Title similarity  \n",
        "    job_title= input_preparation(job[1])\n",
        "    profile_title= input_preparation(profile[1])\n",
        "    result_title= get_sif_feature_vectors(job_title,profile_title,titles_model)\n",
        "    #if any word from job input or profile input could be detected in the vocabulary\n",
        "    if(result_title==0):\n",
        "       matching_title=0\n",
        "    else:\n",
        "       matching_title= get_cosine_similarity(result_title[0],result_title[1])\n",
        "    print('matching_title:',matching_title)\n",
        "  #Description similarity\n",
        "    job_description= input_preparation(job[2])\n",
        "    profile_description= input_preparation(profile[2])\n",
        "    result_description= get_sif_feature_vectors(job_description,profile_description,desciptions_model)\n",
        "     #if any word from job input or profile input could be detected in the vocabulary\n",
        "    if(result_description==0):\n",
        "       matching_description=0\n",
        "    else:\n",
        "      matching_description= get_cosine_similarity(result_description[0],result_description[1]) \n",
        "    print('matching_description:', matching_description)\n",
        "    matchings=[]\n",
        "    matchings.append(matching_title)\n",
        "    matchings.append(matching_description)\n",
        "    matchings.append(matching_skills)\n",
        "    final_matching= matching_skills * skills_rate + matching_title * title_rate + matching_description * description_rate\n",
        "    matchings.append(final_matching)\n",
        "    return (matchings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9_ceqaQ7Dda",
        "colab_type": "text"
      },
      "source": [
        "# Model building and publishing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YpEXFg6W-30",
        "colab_type": "text"
      },
      "source": [
        "Data loading and model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5NBeEyXhP58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the data set from data.world\n",
        "df = pd.read_csv('https://query.data.world/s/mmxnlcbuvbcx5gdrvgllt52zn5wu75')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omA3lN2IuxIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the concerned columns\n",
        "data_skills = df['skills'].apply(lambda x: str(x).lower().replace('see below','').replace('(see job description)','').replace('see job description','').replace('full time','').replace('part time',''))\n",
        "data_jobtitles=df['jobtitle']\n",
        "\n",
        "#Description processing\n",
        "size=len(df['jobdescription'].index) // 7\n",
        "data_description=df['jobdescription'].iloc[0:size]\n",
        "#job_description = df[['jobtitle', 'jobdescription']].apply(lambda x: ' '.join(x), axis = 1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUf2rgtzM7L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Skills Model\n",
        "#data processing\n",
        "skills = data_processing(data_skills)\n",
        "#models vocabs creation\n",
        "model_skills = word2vec_creation(skills,min_count=20, window=2,size=300,sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20)\n",
        "#models training\n",
        "w2v_skills= word2vec_training(skills,model_skills)\n",
        "\n",
        "# Job Titles Model\n",
        "jobtitles = data_processing(data_jobtitles)\n",
        "model_jobtitles = word2vec_creation(jobtitles,min_count=5, window=2,size=300,sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20)\n",
        "w2v_jobtitles= word2vec_training(jobtitles,model_jobtitles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdtplh_kVHxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Description Model\n",
        "descriptions = data_processing(data_description)\n",
        "model_description=word2vec_creation(descriptions,min_count=20, window=2,size=300,sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20)\n",
        "w2v_description= word2vec_training(descriptions,model_description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vljcb8gqkUKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Export models format .p using pickl\n",
        "pickl = {'model': w2v_skills}\n",
        "pickle.dump( pickl, open( 'w2v_skills' + \".p\", \"wb\" ) )\n",
        "\n",
        "pickl = {'model': w2v_jobtitles}\n",
        "pickle.dump( pickl, open( 'w2v_jobtitles' + \".p\", \"wb\" ) )\n",
        "\n",
        "pickl = {'model': w2v_description}\n",
        "pickle.dump( pickl, open( 'w2v_description' + \".p\", \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TuqICzNmucx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Load the models files \n",
        " file_name = \"w2v_skills.p\"\n",
        " with open(file_name, 'rb') as pickled:\n",
        "  data = pickle.load(pickled)\n",
        "  model_skills = data['model']\n",
        "\n",
        "file_name = \"w2v_description.p\"\n",
        " with open(file_name, 'rb') as pickled:\n",
        "  data = pickle.load(pickled)\n",
        "  model_description = data['model']\n",
        "\n",
        "file_name = \"w2v_jobtitles.p\"\n",
        " with open(file_name, 'rb') as pickled:\n",
        "  data = pickle.load(pickled)\n",
        "  model_jobtitles = data['model']  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXLMEPuhXGiK",
        "colab_type": "text"
      },
      "source": [
        "#Tests\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4RhLB-zzjBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test models format .p\n",
        "# skills, jobTitle, description\n",
        "job=[' C++, Development, Programming, Python, Shell Script, Software','Business Intelligence Analyst','Junior to Mid-Level Business Intelligence AnalystLocation: onsite Marlborough, MAUS Citizens and Green Card Holders and those authorized to work in the US are encouraged to apply. We are unable to sponsor H1B candidates at this time.Description:We are looking for an energetic go getter who is looking to climb the career ladder!Would you like to be working with one of the worlds largest travel groups, with more than 16,000 staff worldwide. They are the world leader in the enterprise travel management space with an active global network spanning over 90 countries worldwide.  Offering a unique and welcoming culture, this rapidly growing organization works with cutting edge technology, offers a very stable, career growth-oriented environment and has excellent benefits including discounts on accommodationsWe seek a motivated person to work on the Global Technology team to enhance our technical product platform. This position requires technical ingenuity, the ability to work with people and win the hearts of our great clients. The ideal candidate will have experience working with the Microsoft BI platform and knows how to extract data from complex databases using SQL.The position will require the candidate to liaise with our global teams to manage and assist with projects through to completion. An applicant should be able to demonstrate the ability to initiate process improvements with automation to create efficiencies.  If this job is for you and you do not mind getting your hands dirty, we’ll help you grow your development skills with 2 industry leading BI platforms the Microsoft Business Intelligence Stack and the GoodData reporting suite.Responsibilities:Must have experience with: SQL Server solid understanding of relational database. Able to write queries with joins (basic to mid-level)Solid understanding of TSQL syntax, stored procedures and user defined functionsStrong problem solving and troubleshooting skillsSoft skills: Smart and quick learner.Respond to internal support queries, such as data and report validation questionsAdhoc analysis of travel data for internal review & external clientsDocumenting processes and proceduresAssist with support of our Web Development team Position Requirements:Hands on experience extracting data with SQL using complex joinsExperience using a business intelligence and analytic platformMust have experience working with end users and/ or technical product ownersMust be able to facilitate meetings and keep participants engagedHighly logical with proven analytical and problem-solving abilitiesExperience of gathering user requirements and translating these into codeAbility to execute tasks in a high-pressure environmentStrong interpersonal, written and oral communication skillsExperience of working both independently and in a team environmentStrong technical knowledge of the Microsoft Office SuiteExperience with SQL Server Reporting Services is a plusWorking knowledge of Sharepoint is a plus']\n",
        "# skills, jobTitle, experiences\n",
        "profile=['data science, python, tensorflow','Business  Analyst','Junior to Mid-Level Business Intelligence AnalystLocation: onsite Marlborough, MAUS Citizens and Green Card Holders and those authorized to work in the US are encouraged to apply. We are unable to sponsor H1B candidates at this time.Description:We are looking for an energetic go getter who is looking to climb the career ladder!Would you like to be working with one of the worlds largest travel groups, with more than 16,000 staff worldwide. They are the world leader in the enterprise travel management space with an active global network spanning over 90 countries worldwide.  Offering a unique and welcoming culture, this rapidly growing organization works with cutting edge technology, offers a very stable, career growth-oriented environment and has excellent benefits including discounts on accommodationsWe seek a motivated person to work on the Global Technology team to enhance our technical product platform. This position requires technical ingenuity, the ability to work with people and win the hearts of our great clients. The ideal candidate will have experience working with the Microsoft BI platform and knows how to extract data from complex databases using SQL.The position will require the candidate to liaise with our global teams to manage and assist with projects through to completion. An applicant should be able to demonstrate the ability to initiate process improvements with automation to create efficiencies.  If this job is for you and you do not mind getting your hands dirty, we’ll help you grow your development skills with 2 industry leading BI platforms the Microsoft Business Intelligence Stack and the GoodData reporting suite.Responsibilities:Must have experience with: SQL Server solid understanding of relational database. Able to write queries with joins (basic to mid-level)Solid understanding of TSQL syntax, stored procedures and user defined functionsStrong problem solving and troubleshooting skillsSoft skills: Smart and quick learner.Respond to internal support queries, such as data and report validation questionsAdhoc analysis of travel data for internal review & external clientsDocumenting processes and proceduresAssist with support of our Web Development team Position Requirements:Hands on experience extracting data with SQL using complex joinsExperience using a business intelligence and analytic platformMust have experience working with end users and/ or technical product ownersMust be able to facilitate meetings and keep participants engagedHighly logical with proven analytical and problem-solving abilitiesExperience of gathering user requirements and translating these into codeAbility to execute tasks in a high-pressure environmentStrong interpersonal, written and oral communication skillsExperience of working both independently and in a team environmentStrong technical knowledge of the Microsoft Office SuiteExperience with SQL Server Reporting Services is a plusWorking knowledge of Sharepoint is a plus']\n",
        "matching= job_profile_matching(0.4, 0.5, 0.1, job, profile,model_skills, model_jobtitles, model_description)\n",
        "matching"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}